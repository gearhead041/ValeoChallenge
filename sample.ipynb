{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import necessary libraries and read the input data. We're going to do some explorations on the data to understand it better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#remove the date columns\n",
    "# we can't really use them for anything as pertains to predicting the outcome\n",
    "# at least that's what I think\n",
    "x = pd.read_csv('./traininginputs.csv')\n",
    "x.drop(columns=['PROC_TRACEINFO'],inplace=True)\n",
    "y = pd.read_csv('./trainingoutput.csv')\n",
    "y.drop(columns=['PROC_TRACEINFO'],inplace=True)\n",
    "\n",
    "# we need to convert the Binar OP130_Resultat_Global_v to a boolean\n",
    "y['Binar OP130_Resultat_Global_v'] = y['Binar OP130_Resultat_Global_v'].astype(\n",
    "    'bool')\n",
    "\n",
    "# we're just creating this to explore the data x & y is what will be actualy used\n",
    "training = pd.concat([x, y['Binar OP130_Resultat_Global_v']], axis=1)\n",
    "\n",
    "# training.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the df.corr() function we can see how much the columns are related to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training.corr(numeric_only=True)['Binar OP130_Resultat_Global_v'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No column has a signficantly high correlation with the target. So no liner models for this one.\n",
    "\n",
    "Since there is little correlation, we have to use a non-linear model to predict the target. We will use a Random Forest Classifier, this can handle missing values, which we have and is less prone to overfitting like a single decision tree.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ravel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42, shuffle=True)\n",
    "# tweak these variables to get a better model. Good luck!\n",
    "# you can also look up GridSearchCV to automate this process, ask ChatGPT\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100,\n",
    "                            min_samples_leaf=0.051, max_features=0.76)\n",
    "rf.fit(X_train, ravel(y_train))\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate Model\n",
    "# Get probability scores for the positive class\n",
    "y_probs = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "rf_auc = auc(fpr, tpr)  # Compute AUC Score\n",
    "\n",
    "# should be 0.685, try for 0.7\n",
    "print(f'Random Forest Classifier Accuracy: {rf_auc}')\n",
    "# not so important here, but its 0.742\n",
    "print(f'Random Forest Classifier Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also use precision recall to evaluate the model, since the classes are imbalanced. (there are hardly any NOK parts). That's why we set the class_weights to balanced. This will make the model pay more attention to the NOK parts.\n",
    "\n",
    "## Discussion about the Data\n",
    "The company makes automotive parts and after fabrication, some readings are taken from each part. Those are the columns in the input data, aside from PROC_ID which is the unique ID for each part. At the end of this process, the part is classified as either \"OK\" or \"NOK\".\n",
    "\n",
    "We want to build a model that can predict this classification based on the readings taken from the part without needing that final inspection.\n",
    "\n",
    "Since we used a Random Forest Classifier, we can also see which features are the most important for the model. This can give us some insights into what makes a part NOK. We can use this information to improve the fabrication process and reduce the number of NOK parts. (I have no idea how this would be done, don't ask me).\n",
    "\n",
    "## Ways to imporve accuracy and efficiency\n",
    "- We could try other models like XGBoost or LightGBM\n",
    "- We could try to balance the classes in the data\n",
    "- We could try to use some feature engineering to create new features (I was thinking about extracting dates from the PROC_ID, but I don't know if that would be useful)\n",
    "- We could try to use some feature selection to remove unimportant features (I don't think this is necessary since the model is already fast)\n",
    "- We could try to use some hyperparameter tuning to improve the model (we *should* do this, but I'm lazy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
